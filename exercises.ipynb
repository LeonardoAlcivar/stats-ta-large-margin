{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression \n",
    "from sklearn.svm import LinearSVC\n",
    "from time import sleep\n",
    "import attr\n",
    "import matplotlib.pyplot as plt\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def plot_boundary(decision_function):\n",
    "    xx, yy = np.meshgrid(np.linspace(-2, 10), np.linspace(-6,6))\n",
    "    Z = decision_function(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    ax = sns.scatterplot(x=0, y=1, hue='label', data=data)\n",
    "    ax.contour(xx, yy, Z, levels=[0])\n",
    "\n",
    "def add_intercept(X):\n",
    "    N,_ = X.shape\n",
    "    return np.concatenate([np.ones(N).reshape(N, -1), X], 1)\n",
    "\n",
    "class LossFunction(ABC):\n",
    "    \"\"\" Base class for loss functions \"\"\"\n",
    "    @abstractmethod\n",
    "    def loss(self, p, y):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def dloss(self, p, y):\n",
    "        pass\n",
    "\n",
    "@attr.s\n",
    "class SGD():\n",
    "    \"\"\"\n",
    "    Simple stochastic gradient descent!\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    loss: LossFunction\n",
    "    \tAn instance of the LossFunction class\n",
    "\n",
    "    eta : float\n",
    "    \tLearning rate for descent\n",
    "\n",
    "    w_init : array, shape(n_features)\n",
    "    \tInitial weights\n",
    "\n",
    "    max_iter: int\n",
    "    \tMaximum number of epochs to train\n",
    "    \n",
    "    tol: float\n",
    "    \tTolerance for stopping from convergence\n",
    "\n",
    "\n",
    "    Attributes\n",
    "\n",
    "    weights: array, shape(n_features)\n",
    "    \tThe fitted weights of the model\n",
    "\n",
    "    losses: array, shape(n_epochs)\n",
    "    \tThe losses over time\n",
    "    \"\"\"\n",
    "\n",
    "    loss = attr.ib()\n",
    "    eta = attr.ib()\n",
    "    w_init = attr.ib(default=None)\n",
    "    max_iter = attr.ib(default=1000)\n",
    "    tol = attr.ib(default=1e-9)\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        X = add_intercept(X)\n",
    "        return X.dot(self.weights)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        X : array, shape(n_samples, n_features)\n",
    "            Your data, without intercept\n",
    "\n",
    "        y : array, shape (n_samples)\n",
    "            Your labels, in {+1,-1}\n",
    "       \n",
    "        Returns\n",
    "        -------\n",
    "        \n",
    "        weights: array, shape(n_features)\n",
    "    \t    The fitted weights of the model\n",
    "        \"\"\"\n",
    "\n",
    "        X = add_intercept(X)\n",
    "        N,P = X.shape\n",
    "\n",
    "        # init weights\n",
    "        w = self.w_init\n",
    "        if w is None:\n",
    "            w = np.random.normal(0,1,P)\n",
    "\n",
    "        iters = 0\n",
    "        self.losses = []\n",
    "\n",
    "        for j in range(self.max_iter):\n",
    "            tot_loss = 0\n",
    "            iters += 1\n",
    "            w_new = w.copy()\n",
    "\n",
    "            # Stochastic part!\n",
    "            idx = np.arange(N)\n",
    "            np.random.shuffle(idx)\n",
    "            # Loss of each data point\n",
    "            for i in idx:\n",
    "                p = np.dot(X[i], w)\n",
    "                tot_loss += self.loss.loss(p, y[i])\n",
    "                dloss = self.loss.dloss(p, y[i], w_new)\n",
    "                update = X[i] * (-self.eta * dloss)\n",
    "                w_new += update\n",
    "\n",
    "            # Check convergence\n",
    "            if np.linalg.norm(w_new - w) < self.tol:\n",
    "                break\n",
    "            else:\n",
    "                w = w_new\n",
    "\n",
    "            self.losses.append(tot_loss)\n",
    "        print('Finished after {} iterations. Finall loss = {}'.format(iters, tot_loss))\n",
    "        self.weights = w\n",
    "        return w    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Generate some data to play with! This is important to be able to do. \n",
    "# Generate a couple of classes, try with linearly separable and not linearly\n",
    "# separable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Implement the different loss functions (as classes that inherit\n",
    "# from LossFunction. For example: Logistic, Hinge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Use the SGD classifier from above with the data and different loss\n",
    "# functions. See how it works! \n",
    "# Play with max_iter, and look at the losses and convergence. \n",
    "# How do the loss functions affect things? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Try the other loss functions from the slides. \n",
    "# Does the square loss converge well? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Try with LogisticRegression from Sklearn using the newton method\n",
    "# Does it converge faster? That's second-order optimization! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "name": "exercises.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
